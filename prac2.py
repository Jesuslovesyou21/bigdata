# -*- coding: utf-8 -*-
"""prac2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jiL7ye-uHq4jMLDC9D_gHvzKqLcD3MQL
"""

# âœ… í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from datasets import load_dataset, concatenate_datasets
from transformers import BertTokenizerFast, BertForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW
from torch.utils.data import TensorDataset, DataLoader, RandomSampler
import torch

# âœ… 1. IMDB ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹ ë¡œë“œ
# IMDBëŠ” 'train'ê³¼ 'test'ë¡œ ë‚˜ë‰œ binary classificationìš© ê°ì„± ë°ì´í„°ì…‹
dataset = load_dataset("imdb")

# âœ… 2. ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ê· í˜•ì¡íŒ ìƒ˜í”Œë§
# ì•„ë˜ ë°©ì‹ìœ¼ë¡œ ë°ì´í„° í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì„ íƒ í•  ê²½ìš° ë°ì´í„° í¸í–¥ìœ¼ë¡œ ì¸í•œ ê³¼ì í•© ë˜ëŠ” ì˜¤ì í•© ë°œìƒ
# train_data = dataset["train"].select(range(2000))  # ë©”ëª¨ë¦¬ ì´ˆê³¼ ë°©ì§€ìš©
# test_data = dataset["test"].select(range(500))     # ë©”ëª¨ë¦¬ ì´ˆê³¼ ë°©ì§€ìš©
# ì›ë³¸ train ë°ì´í„°ëŠ” ë¶€ì •(0) ë¦¬ë·°ê°€ ì•ìª½ì— ëª°ë ¤ ìˆìŒ â‡’ ëª¨ë¸ í¸í–¥ ìœ ë°œ
# ë”°ë¼ì„œ ë¶€ì •/ê¸ì • ê°ê° 5000ê°œì”© ë¬´ì‘ìœ„ ì¶”ì¶œí•˜ì—¬ ê· í˜• ìœ ì§€
neg_samples = dataset["train"].filter(lambda x: x['label'] == 0).shuffle(seed=42).select(range(5000))
pos_samples = dataset["train"].filter(lambda x: x['label'] == 1).shuffle(seed=42).select(range(5000))

# ë¶€ì •/ê¸ì • ê²°í•© í›„ ë‹¤ì‹œ ì…”í”Œí•˜ì—¬ train ë°ì´í„° êµ¬ì„±
balanced_dataset = concatenate_datasets([neg_samples, pos_samples]).shuffle(seed=42)

# âœ… 3. í•™ìŠµ/í‰ê°€ìš© ë°ì´í„° ë¶„ë¦¬
# í•™ìŠµ ë°ì´í„°: ê· í˜• ìƒ˜í”Œ ì¤‘ 2,000ê°œ ì‚¬ìš© (Colab í™˜ê²½ ê³ ë ¤)
# í…ŒìŠ¤íŠ¸ ë°ì´í„°: ì›ë³¸ IMDB test ë°ì´í„° ì¤‘ 500ê°œ ì‚¬ìš©
train_data = balanced_dataset.select(range(2000))
test_data = load_dataset("imdb")["test"].select(range(500))

# âœ‚ï¸ í›ˆë ¨ ë°ì´í„°ì…‹ì—ì„œ ë¦¬ë·° í…ìŠ¤íŠ¸ì™€ ë¼ë²¨ë§Œ ë¶„ë¦¬í•˜ì—¬ ë³„ë„ ë³€ìˆ˜ì— ì €ì¥
train_texts = train_data["text"]   # ë¦¬ë·° ë³¸ë¬¸ í…ìŠ¤íŠ¸ ëª©ë¡ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
train_labels = train_data["label"] # ê°ì„± ë ˆì´ë¸” (0: ë¶€ì •, 1: ê¸ì •)

# ğŸ”¤ ì‚¬ì „í•™ìŠµëœ BERTìš© í† í¬ë‚˜ì´ì € ë¡œë“œ (WordPiece ê¸°ë°˜ í† í°í™”)
# 'bert-base-uncased'ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ì²˜ë¦¬í•¨
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

# âœ‚ï¸ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì • (ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ìë¥´ê³ , ì§§ì€ ë¬¸ì¥ì€ íŒ¨ë”©í•¨)
MAX_LEN = 256

# ğŸ§ª í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ë“¤ì„ BERT ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
# truncation=True â†’ ê¸¸ë©´ ìë¥´ê¸° / padding='max_length' â†’ ì§§ìœ¼ë©´ 256ê¸¸ì´ì— ë§ì¶° íŒ¨ë”©
# return_tensors="pt" â†’ PyTorch í…ì„œ í˜•íƒœë¡œ ë°˜í™˜
encodings = tokenizer(
    train_texts,
    truncation=True,
    padding="max_length",
    max_length=MAX_LEN,
    return_tensors="pt"
)

# ğŸ”  ì¸ì½”ë”© ê²°ê³¼ì—ì„œ input_idsì™€ attention_mask ì¶”ì¶œ
# input_ids: í† í°ì˜ ì •ìˆ˜ ì¸ë±ìŠ¤
# attention_mask: íŒ¨ë”©ì´ ì•„ë‹Œ ë¶€ë¶„ì€ 1, íŒ¨ë”©ëœ ë¶€ë¶„ì€ 0
train_inputs = encodings["input_ids"]
train_masks = encodings["attention_mask"]

# ğŸ·ï¸ ë¼ë²¨ë„ torch í…ì„œ í˜•íƒœë¡œ ë³€í™˜ (ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•˜ê¸° ìœ„í•¨)
train_labels = torch.tensor(train_labels)

# ğŸ§³ PyTorch Dataset ë° Dataloader êµ¬ì„±

# í•œ ë²ˆì— í•™ìŠµí•  ë°ì´í„° ìˆ˜ ì„¤ì • (ë¯¸ë‹ˆ ë°°ì¹˜ í¬ê¸°)
batch_size = 16

# ğŸ“¦ TensorDataset: ì…ë ¥ ë°ì´í„°(input_ids), ë§ˆìŠ¤í¬(attention_mask), ì •ë‹µ ë¼ë²¨(labels)ì„ í•˜ë‚˜ì˜ Datasetìœ¼ë¡œ ë¬¶ìŒ
# => í•™ìŠµ ì‹œ ê° ì¸ë±ìŠ¤ì— ëŒ€í•´ (input_ids, attention_mask, label)ì„ ë°˜í™˜
train_dataset = TensorDataset(train_inputs, train_masks, train_labels)

# ğŸšš DataLoader: ì‹¤ì œ í•™ìŠµì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ê³µê¸‰í•´ì£¼ëŠ” ì—­í• 
# RandomSampler: ë§¤ epochë§ˆë‹¤ ë°ì´í„°ë¥¼ **ë¬´ì‘ìœ„ë¡œ ì„ì–´** ìƒ˜í”Œë§ â†’ í•™ìŠµ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€
# batch_size: í•œ ë°°ì¹˜ì— í¬í•¨ë  ìƒ˜í”Œ ìˆ˜
train_dataloader = DataLoader(
    train_dataset,                     # í•™ìŠµí•  ë°ì´í„°ì…‹
    sampler=RandomSampler(train_dataset),  # ëœë¤ ìƒ˜í”Œë§ (epochë§ˆë‹¤ ë°ì´í„° ìˆœì„œ ì„ê¸°)
    batch_size=batch_size             # í•œ ë²ˆì— ì½ì–´ì˜¬ ìƒ˜í”Œ ê°œìˆ˜ (ë°°ì¹˜ ë‹¨ìœ„)
)

# ğŸ”§ GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
# CUDAê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPU ì‚¬ìš©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ CPU ì‚¬ìš©
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ§  ì‚¬ì „í•™ìŠµëœ BERT ë¶„ë¥˜ ëª¨ë¸ ë¡œë”©
# 'bert-base-uncased': ì†Œë¬¸ìí™”ëœ BERT ì‚¬ì „í•™ìŠµ ëª¨ë¸
# num_labels=2: ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ (ê¸ì •/ë¶€ì •)
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# ëª¨ë¸ì„ ì„ íƒí•œ ì¥ì¹˜(GPU ë˜ëŠ” CPU)ë¡œ ì´ë™
model.to(device)

# ğŸ” ì˜µí‹°ë§ˆì´ì € ë° í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
# AdamW: BERT ë…¼ë¬¸ì—ì„œ ê¶Œì¥í•˜ëŠ” ì˜µí‹°ë§ˆì´ì € (ê°€ì¤‘ì¹˜ ê°ì‡  ì ìš©)
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)

# ì´ í•™ìŠµ epoch ìˆ˜ ì„¤ì •
epochs = 2

# ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜ ê³„ì‚° (ì´ ë°°ì¹˜ ìˆ˜ Ã— epoch ìˆ˜)
total_steps = len(train_dataloader) * epochs

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
# Warm-up ì—†ì´ ì„ í˜• ê°ì†Œ ë°©ì‹ìœ¼ë¡œ í•™ìŠµë¥  ì¡°ì •
scheduler = get_linear_schedule_with_warmup(
    optimizer,                 # ì‚¬ìš©í•  ì˜µí‹°ë§ˆì´ì €
    num_warmup_steps=0,        # warm-up ë‹¨ê³„ ì—†ìŒ
    num_training_steps=total_steps  # ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜
)

import time
import datetime
import numpy as np
import random

# ğŸ§¬ ëœë¤ ì‹œë“œ ê³ ì • (ê²°ê³¼ ì¬í˜„ ê°€ëŠ¥ì„±ì„ ë†’ì´ê¸° ìœ„í•¨)
seed_val = 777
random.seed(seed_val)                    # íŒŒì´ì¬ random ê³ ì •
np.random.seed(seed_val)                # ë„˜íŒŒì´ random ê³ ì •
torch.manual_seed(seed_val)             # PyTorch random ê³ ì •
torch.cuda.manual_seed_all(seed_val)    # CUDAì—ì„œë„ ë™ì¼í•˜ê²Œ ì‹œë“œ ê³ ì •

# í•™ìŠµ ì‹œê°„ í¬ë§· í•¨ìˆ˜ (ì´ˆ â†’ ì‹œ:ë¶„:ì´ˆ)
def format_time(elapsed):
    return str(datetime.timedelta(seconds=int(round(elapsed))))

# ğŸ‹ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘
model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜ (Dropout, BatchNorm ë“±ì´ í•™ìŠµ ëª¨ë“œë¡œ ì‘ë™)

# ê° epoch ë°˜ë³µ
for epoch_i in range(epochs):
    print(f"\n======== Epoch {epoch_i + 1} / {epochs} ========")
    t0 = time.time()  # í•™ìŠµ ì‹œì‘ ì‹œê°„ ì €ì¥
    total_loss = 0    # ëˆ„ì  ì†ì‹¤ ì´ˆê¸°í™”

    # ë¯¸ë‹ˆë°°ì¹˜ ë°˜ë³µ
    for step, batch in enumerate(train_dataloader):
        # ë§¤ 100ìŠ¤í…ë§ˆë‹¤ ê²½ê³¼ ì‹œê°„ ì¶œë ¥
        if step % 100 == 0:
            elapsed = format_time(time.time() - t0)
            print(f"  â–¶ Step {step}/{len(train_dataloader)} - Elapsed: {elapsed}")

        # ì…ë ¥ ë°ì´í„°ì™€ ë¼ë²¨ì„ ì¥ì¹˜(GPU/CPU)ë¡œ ì´ë™
        b_input_ids, b_input_mask, b_labels = [b.to(device) for b in batch]

        model.zero_grad()  # ê¸°ìš¸ê¸° ì´ˆê¸°í™”

        # ğŸ” ìˆœì „íŒŒ â†’ ì†ì‹¤ ê³„ì‚°
        outputs = model(
            b_input_ids,
            attention_mask=b_input_mask,
            labels=b_labels
        )
        loss = outputs.loss         # CrossEntropyLoss í¬í•¨
        total_loss += loss.item()   # ì†ì‹¤ ëˆ„ì 

        # ğŸ”„ ì—­ì „íŒŒ â†’ ê°€ì¤‘ì¹˜ ê°±ì‹ 
        loss.backward()  # ì†ì‹¤ ê¸°ì¤€ìœ¼ë¡œ ì—­ì „íŒŒ ìˆ˜í–‰

        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘: exploding gradient ë°©ì§€
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        scheduler.step()  # í•™ìŠµë¥  ì¡°ì •

    # â± ì—í­ ë‹¨ìœ„ í‰ê·  ì†ì‹¤ ì¶œë ¥
    avg_loss = total_loss / len(train_dataloader)
    print(f"âœ… Epoch {epoch_i+1} Avg Loss: {avg_loss:.4f}")
    print(f"ğŸ•’ Epoch time: {format_time(time.time() - t0)}")

from transformers import pipeline

# ğŸ“ GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ â†’ GPUê°€ ìˆìœ¼ë©´ 0, ì—†ìœ¼ë©´ CPU(-1)ë¡œ ì„¤ì •
device_id = 0 if torch.cuda.is_available() else -1

# ğŸ¤– í•™ìŠµëœ ëª¨ë¸ë¡œ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
clf_pipeline = pipeline(
    task="text-classification",        # ì‘ì—… ìœ í˜•: ë¬¸ì¥ ë¶„ë¥˜
    model=model,                       # fine-tuningëœ BERT ë¶„ë¥˜ ëª¨ë¸
    tokenizer=tokenizer,               # ê°™ì€ baseì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©
    device=device_id,                  # ë””ë°”ì´ìŠ¤ ì„¤ì •: GPU or CPU
    return_all_scores=True,            # ë‘ í´ë˜ìŠ¤ì˜ í™•ë¥  ëª¨ë‘ ì¶œë ¥
    function_to_apply="softmax"        # ì¶œë ¥ ë¡œì§“ â†’ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜
)

# ğŸ” ì˜ˆì¸¡í•  í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
sample_texts = [
    "This movie was absolutely fantastic. I loved the plot and the characters.",   # ê¸ì •
    "It was the worst film I have ever seen. Completely boring.",                  # ë¶€ì •
    "Not bad, but not great either. Just okay."                                    # ì¤‘ë¦½ì— ê°€ê¹Œì›€ (íŒë‹¨ ì• ë§¤)
]

# ğŸ§¾ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ì˜ˆì¸¡ ìˆ˜í–‰
predictions = clf_pipeline(sample_texts)

# ğŸ“‹ ê° ë¬¸ì¥ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
for text, result in zip(sample_texts, predictions):
    print(f"\nğŸ“Œ ì…ë ¥ ë¬¸ì¥: {text}")
    for label in result:
        print(f"  {label['label']} â†’ {label['score']:.4f}")  # label: POSITIVE/NEGATIVE, score: í™•ë¥ ê°’

Device set to use cuda:0

ğŸ“Œ ì…ë ¥ ë¬¸ì¥: This movie was absolutely fantastic. I loved the plot and the characters.
  LABEL_0 â†’ 0.0199
  LABEL_1 â†’ 0.9801

ğŸ“Œ ì…ë ¥ ë¬¸ì¥: It was the worst film I have ever seen. Completely boring.
  LABEL_0 â†’ 0.9772
  LABEL_1 â†’ 0.0228

ğŸ“Œ ì…ë ¥ ë¬¸ì¥: Not bad, but not great either. Just okay.
  LABEL_0 â†’ 0.3401
  LABEL_1 â†’ 0.6599

from sklearn.metrics import accuracy_score, f1_score

# ğŸ¯ í…ŒìŠ¤íŠ¸ì…‹ ì „ì²˜ë¦¬
test_texts = test_data["text"]             # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
test_labels = test_data["label"]           # í…ŒìŠ¤íŠ¸ ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸

# ğŸ§ª í† í¬ë‚˜ì´ì§• ë° ì¸ì½”ë”© (í›ˆë ¨ê³¼ ë™ì¼í•˜ê²Œ max_length ê¸°ì¤€ padding/truncation ìˆ˜í–‰)
test_encodings = tokenizer(
    test_texts,
    truncation=True,
    padding="max_length",
    max_length=MAX_LEN,
    return_tensors="pt"                    # PyTorch í…ì„œ í˜•íƒœë¡œ ë°˜í™˜
)

test_inputs = test_encodings["input_ids"]         # ì…ë ¥ í† í° ID
test_masks = test_encodings["attention_mask"]     # ì–´í…ì…˜ ë§ˆìŠ¤í¬
test_labels_tensor = torch.tensor(test_labels)    # ì •ë‹µ ë ˆì´ë¸”

# ğŸ“¦ PyTorchìš© í…ŒìŠ¤íŠ¸ Dataset ë° DataLoader êµ¬ì„±
test_dataset = TensorDataset(test_inputs, test_masks, test_labels_tensor)
test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)

# ğŸ“ˆ í‰ê°€ í•¨ìˆ˜ ì •ì˜
def evaluate_model(model, dataloader):
    model.eval()              # í‰ê°€ ëª¨ë“œ (Dropout/BatchNorm ë“± ë¹„í™œì„±í™”)
    preds = []                # ì˜ˆì¸¡ê°’ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    true_labels = []          # ì‹¤ì œê°’ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸

    for batch in dataloader:
        # ë¯¸ë‹ˆë°°ì¹˜ ë°ì´í„°ë¥¼ GPU/CPUë¡œ ì´ë™
        b_input_ids, b_input_mask, b_labels = [b.to(device) for b in batch]

        with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ì†ë„â†‘, ë©”ëª¨ë¦¬â†“)
            outputs = model(b_input_ids, attention_mask=b_input_mask)

        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1).cpu().numpy()  # ê°€ì¥ í™•ë¥  ë†’ì€ í´ë˜ìŠ¤ ì„ íƒ
        labels = b_labels.cpu().numpy()

        preds.extend(predictions)      # ì˜ˆì¸¡ ê²°ê³¼ ëˆ„ì 
        true_labels.extend(labels)     # ì‹¤ì œ ë ˆì´ë¸” ëˆ„ì 

    return preds, true_labels

# ğŸ§ª í‰ê°€ ì‹¤í–‰
preds, true_labels = evaluate_model(model, test_dataloader)

# âœ… ì •í™•ë„ ë° F1 ì ìˆ˜ ê³„ì‚° ë° ì¶œë ¥
acc = accuracy_score(true_labels, preds)                         # ì •í™•ë„: ì˜ˆì¸¡ê³¼ ì‹¤ì œ ì¼ì¹˜ ë¹„ìœ¨
f1 = f1_score(true_labels, preds, average='weighted')            # F1 ì ìˆ˜: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· 

print(f"\nâœ… Test Accuracy: {acc:.4f}")
print(f"âœ… Test F1 Score: {f1:.4f}")

