# -*- coding: utf-8 -*-
"""ë¹…ë°ì´í„°ê°œë¡  14ì£¼ì°¨ ê³¼ì œ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bpkl273IH-LLALz5NPFK8y5hrD7q0yaG
"""

# ğŸ“¦ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import time
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import pipeline
import torch

# ğŸ“Œ Step 1: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
print("ğŸ”¹ Loading 20 Newsgroups dataset...")
newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))
newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))

vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, max_features=5000)
X_train = vectorizer.fit_transform(newsgroups_train.data)
X_test = vectorizer.transform(newsgroups_test.data)

y_train = newsgroups_train.target
y_test = newsgroups_test.target

# ğŸ“Œ Step 2: ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ

# âœ… Naive Bayes
print("ğŸ”¹ Training Naive Bayes...")
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# âœ… SGDClassifier (Linear SVM)
print("ğŸ”¹ Training SGDClassifier...")
sgd_model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3)
sgd_model.fit(X_train, y_train)

# âœ… BERT íŒŒì´í”„ë¼ì¸ ë¡œë“œ
print("ğŸ”¹ Loading BERT pipeline...")
device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english", device=device)

# ğŸ“Œ Step 3: ì˜ˆì¸¡

# ğŸ”¸ Naive Bayes ì˜ˆì¸¡
y_pred_nb = nb_model.predict(X_test)

# ğŸ”¸ SGDClassifier ì˜ˆì¸¡
y_pred_sgd = sgd_model.predict(X_test)

# ğŸ”¸ BERT ì˜ˆì¸¡ (200ê°œ ë¬¸ì¥ë§Œ ë°°ì¹˜ ì²˜ë¦¬)
print("ğŸ”¹ Running BERT on 200 samples...")
sample_texts = newsgroups_test.data[:200]
true_labels_bert = y_test[:200]

bert_outputs = classifier(
    sample_texts,
    truncation=True,
    padding=True,
    max_length=512,
    batch_size=16
)

bert_predictions = [0 if output['label'] == 'LABEL_0' else 1 for output in bert_outputs]

# ğŸ“Œ Step 4: ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜
def evaluate(name, y_true, y_pred):
    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    print(f"\nâœ… {name} ì„±ëŠ¥")
    print(f"  Accuracy : {acc:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall   : {recall:.4f}")
    print(f"  F1-score : {f1:.4f}")
    return acc, precision, recall, f1

# ğŸ“Œ Step 5: ì„±ëŠ¥ ì¶œë ¥
evaluate("Naive Bayes", y_test, y_pred_nb)
evaluate("SGDClassifier", y_test, y_pred_sgd)
evaluate("BERT (200 samples)", true_labels_bert, bert_predictions)