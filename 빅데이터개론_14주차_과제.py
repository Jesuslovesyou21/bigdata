# -*- coding: utf-8 -*-
"""빅데이터개론 14주차 과제.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bpkl273IH-LLALz5NPFK8y5hrD7q0yaG
"""

# 라이브러리 임포트
import time
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import pipeline
import torch

# 데이터 로드 및 전처리
print("Loading 20 Newsgroups dataset...")
newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))
newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))

vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, max_features=5000)
X_train = vectorizer.fit_transform(newsgroups_train.data)
X_test = vectorizer.transform(newsgroups_test.data)

y_train = newsgroups_train.target
y_test = newsgroups_test.target

# 세가지 모델 정의 및 학습

# Naive Bayes
print("Training Naive Bayes...")
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# SGDClassifier
print("Training SGDClassifier...")
sgd_model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3)
sgd_model.fit(X_train, y_train)

# BERT
print("Loading BERT pipeline...")
device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english", device=device)

# 예측

# Naive Bayes 예측
y_pred_nb = nb_model.predict(X_test)

# SGDClassifier 예측
y_pred_sgd = sgd_model.predict(X_test)

# BERT 예측
print("Running BERT on 200 samples...")
sample_texts = newsgroups_test.data[:200]
true_labels_bert = y_test[:200]

bert_outputs = classifier(
    sample_texts,
    truncation=True,
    padding=True,
    max_length=512,
    batch_size=16
)

bert_predictions = [0 if output['label'] == 'LABEL_0' else 1 for output in bert_outputs]

# 성능 평가
def evaluate(name, y_true, y_pred):
    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
    print(f"\n {name} 성능")
    print(f"  Accuracy : {acc:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall   : {recall:.4f}")
    print(f"  F1-score : {f1:.4f}")
    return acc, precision, recall, f1

evaluate("Naive Bayes", y_test, y_pred_nb)
evaluate("SGDClassifier", y_test, y_pred_sgd)
evaluate("BERT (200 samples)", true_labels_bert, bert_predictions)